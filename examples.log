RAG ENGINE EXAMPLES TEST RUN
Started: 2025-08-16T10:53:46.187289
Python: 3.13.6 (main, Aug  7 2025, 00:00:00) [GCC 15.2.1 20250808 (Red Hat 15.2.1-1)]
Working Directory: /home/yuval/Documents/Projects/RAG
Examples to run: 13
================================================================================

================================================================================
RUNNING: example_advanced_indexing.py
TIME: 2025-08-16T10:53:46.187318
================================================================================
STDOUT:
üèóÔ∏è  Advanced Indexing Strategies - Comprehensive Demo
This demo showcases all advanced indexing capabilities
‚úÖ GOOGLE_API_KEY found - full functionality available!

üìö Created 10 sample documents for indexing

============================================================
 IndexingManager Basics
============================================================
üöÄ Initializing IndexingManager...
üìã Available Indexing Strategies: basic, multi_representation, colbert, raptor
üéØ Active Strategy: basic

üìä Strategy Information:
   basic: ‚úÖ Active, üí§ Not Initialized
   multi_representation: ‚ö™ Available, üí§ Not Initialized
   colbert: ‚ö™ Available, üí§ Not Initialized
   raptor: ‚ö™ Available, üí§ Not Initialized

============================================================
 Basic Indexing Strategy
============================================================
üìö Indexing documents with Basic strategy...
‚úÖ Basic indexing completed successfully!
üìä Indexed 10 documents
üìà Strategy Info: {'strategy': 'basic', 'is_active': True, 'is_initialized': True, 'document_count': 10, 'indexer_class': 'BasicIndexer', 'chunk_count': 10}
üîç Basic retriever available for search operations

============================================================
 Multi-Representation Indexing
============================================================
üîÑ Indexing documents with Multi-Representation strategy...
This strategy creates summaries for retrieval while storing full documents for generation.
‚úÖ Multi-representation indexing completed!
üìä Created 5 summaries for 5 documents
üîç Multi-vector retriever available
   ‚Üí Searches over summaries, returns full documents

============================================================
 ColBERT Token-Level Indexing
============================================================
üéØ Indexing documents with ColBERT strategy...
This strategy provides token-level precision for fine-grained retrieval.
üîß ColBERT readiness: ‚ùå Not Ready
üí° ColBERT requires 'ragatouille' package: uv add ragatouille

============================================================
 RAPTOR Hierarchical Indexing
============================================================
üå≥ Indexing documents with RAPTOR strategy...
This strategy creates hierarchical tree structures with clustered summaries.
‚úÖ RAPTOR indexing completed!
üìä RAPTOR Tree Structure:
   Total Nodes: 11
   Tree Levels: 4
   Root Nodes: 4
   Level 0: 10 nodes
   Level 1: 4 nodes
   Level 2: 4 nodes
   Level 3: 4 nodes
üîç RAPTOR hierarchical search available
   ‚Üí Supports level-specific and hierarchical search

============================================================
 Strategy Comparison
============================================================
üìä Comparing indexing strategies...

üîÑ Testing basic strategy...
   ‚úÖ Success: 3 documents indexed

üîÑ Testing multi_representation strategy...
   ‚úÖ Success: 3 documents indexed

üìã Strategy Comparison Results:
Strategy             Status     Documents  Ready   
--------------------------------------------------
basic                ‚úÖ Success  3          ‚úÖ       
multi_representation ‚úÖ Success  3          ‚úÖ       

============================================================
 Advanced Features
============================================================
üîß Advanced IndexingManager capabilities...

üîç Strategy Readiness Check:
   basic: ‚úÖ Ready
   multi_representation: ‚úÖ Ready
   colbert: ‚ùå Not Ready
   raptor: ‚úÖ Ready

üßπ Clearing all indexes...
‚úÖ All indexes cleared successfully

üîÑ Strategy Switching:
   Original: multi_representation
   ‚úÖ Switched to: multi_representation
   üîô Restored to: multi_representation

============================================================
 Error Handling
============================================================
‚ö†Ô∏è  Testing error handling...
‚úÖ Properly caught invalid strategy: ValueError
‚úÖ Properly rejected invalid strategy switch
‚úÖ Properly handled empty document list

============================================================
 Demo Complete
============================================================
‚úÖ All indexing strategy demonstrations completed!

üí° Key Takeaways:
  ‚Ä¢ 4 advanced indexing strategies available
  ‚Ä¢ Unified IndexingManager for strategy coordination
  ‚Ä¢ Each strategy optimized for different use cases
  ‚Ä¢ Comprehensive error handling and monitoring
  ‚Ä¢ Easy integration with existing RAG systems

üéØ Strategy Selection Guide:
  ‚Ä¢ Basic: Simple chunking for general use
  ‚Ä¢ Multi-Representation: Summaries for retrieval, full docs for generation
  ‚Ä¢ ColBERT: Token-level precision for fine-grained search
  ‚Ä¢ RAPTOR: Hierarchical organization for multi-level retrieval

üöÄ Next Steps:
  1. Choose appropriate strategy for your use case
  2. Configure strategy parameters for optimal performance
  3. Integrate with your document processing pipeline
  4. Monitor performance and adjust as needed

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Invalid strategy: nonexistent_strategy. Available: ['basic', 'multi_representation', 'colbert', 'raptor']
No documents provided for indexing

RETURN CODE: 0

COMPLETED: 2025-08-16T10:54:00.072276
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_advanced_reranking.py
TIME: 2025-08-16T10:54:01.072442
================================================================================
STDOUT:
Advanced Document Re-ranking Demonstration
==================================================

=== Basic Retrieval (No Re-ranking) ===
Query: Python programming tutorial for beginners
Retrieved 3 documents:
1. [python_doc] Python is a high-level programming language known for its simplicity and readability. It's widely us...
   Topic: programming
2. [js_doc] JavaScript is a dynamic programming language primarily used for web development. It enables interact...
   Topic: programming
3. [ml_doc] Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...
   Topic: ai

=== LLM-Based Re-ranking ===
Note: LLM re-ranking requires API keys. Error: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
This demonstrates the structure - in production, configure your Google API key.

=== Complete RetrievalEngine Workflow ===
Query: machine learning algorithms

1. Retrieving from all documents:
   1. [python_doc] Topic: programming
   2. [js_doc] Topic: programming
   3. [ml_doc] Topic: ai

2. Retrieving from AI-specific documents:
   1. [ml_doc] Topic: ai
   2. [nlp_doc] Topic: ai

3. Retrieving with scores:
   1. [python_doc] Score: 0.90 Topic: programming
   2. [js_doc] Score: 0.80 Topic: programming
   3. [ml_doc] Score: 0.70 Topic: ai

4. Enabling LLM re-ranking:
   Re-ranking enabled successfully
   Re-ranking strategy: None
   (Would use Google Gemini for scoring in production)

5. Engine Statistics:
   Available retrievers: ['vector', 'keyword', 'all_docs', 'programming', 'ai']
   Default retriever: vector
   Re-ranking enabled: False
   Re-ranking strategy: None

=== Configuration Management ===
Initial config - Retrieval K: 3, Re-ranking: False
Updating configuration...
Updated config - Retrieval K: 5, Re-ranking: True
Configuration propagated to all components:
  - Retrieval K: 5
  - Re-ranker Top K: 3
  - Use Re-ranking: True

Demonstration complete!

Key Features Demonstrated:
‚úì Basic document retrieval
‚úì LLM-based re-ranking with Google Gemini
‚úì Multiple retriever management
‚úì Retrieval with relevance scores
‚úì Dynamic re-ranking enable/disable
‚úì Configuration management and updates
‚úì Engine statistics and monitoring

For production use:
- Configure Google API key for Gemini
- Set up proper vector stores (Chroma, etc.)
- Configure embedding providers
- Enable logging and monitoring

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Failed to enable re-ranking: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
Failed to setup re-ranker: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

RETURN CODE: 0

COMPLETED: 2025-08-16T10:54:38.585991
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_comprehensive_evaluation.py
TIME: 2025-08-16T10:54:39.586176
================================================================================
STDOUT:
üîç COMPREHENSIVE RAG EVALUATION FRAMEWORK DEMONSTRATION
================================================================================
This example demonstrates the comprehensive evaluation framework
that can coordinate multiple evaluation approaches for RAG systems.

üìã Features demonstrated:
‚Ä¢ Custom LLM-based evaluation with faithfulness and correctness metrics
‚Ä¢ DeepEval integration for standardized evaluation
‚Ä¢ RAGAS integration for RAG-specific metrics
‚Ä¢ Comprehensive evaluation manager with detailed reporting
‚Ä¢ Metrics aggregation and framework comparison
‚Ä¢ Performance analysis and intelligent recommendations
================================================================================
INDIVIDUAL EVALUATOR DEMONSTRATIONS
================================================================================

1. CUSTOM EVALUATOR
----------------------------------------
‚ùå Custom evaluator failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

2. DEEPEVAL INTEGRATION
----------------------------------------
‚ö†Ô∏è  DeepEval not available: DeepEval is not installed. Please install it with: pip install deepeval
üí° Install with: pip install deepeval

3. RAGAS INTEGRATION
----------------------------------------
‚ö†Ô∏è  RAGAS not available: RAGAS is not installed. Please install it with: pip install ragas datasets
üí° Install with: pip install ragas datasets

================================================================================
COMPREHENSIVE EVALUATION MANAGER DEMONSTRATION
================================================================================

üöÄ Initializing Evaluation Manager...
‚ùå Evaluation manager demonstration failed: No evaluation frameworks could be initialized

================================================================================
METRICS COLLECTOR DEMONSTRATION
================================================================================

üìä AGGREGATED METRICS
----------------------------------------
Faithfulness: 0.875
Correctness: 0.795

üîß FRAMEWORK COMPARISON
----------------------------------------
CUSTOM:
  Status: success
  Overall Score: 0.85
  Execution Time: 1.50s
  Test Cases: 1
DEEPEVAL:
  Status: success
  Overall Score: 0.82
  Execution Time: 2.10s
  Test Cases: 1
RAGAS:
  Status: error
  Overall Score: 0.00
  Execution Time: 0.50s
  Test Cases: 0
  Error: Connection timeout

================================================================================
‚úÖ DEMONSTRATION COMPLETE
================================================================================
üöÄ To use in production:
1. Set up your LLM API keys (Google Gemini for custom evaluation)
2. Install optional frameworks: pip install deepeval ragas datasets
3. Create your test cases and RAG responses
4. Initialize EvaluationManager and run comprehensive evaluation
5. Analyze the detailed reports and implement recommendations

üìñ For more examples, see the other files in the examples/ directory

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
WARNING:root:DeepEval not available. Install with: pip install deepeval
WARNING:root:RAGAS not available. Install with: pip install ragas datasets
ERROR:root:Failed to initialize custom evaluator: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
WARNING:root:DeepEval requested but not available
WARNING:root:RAGAS requested but not available
Traceback (most recent call last):
  File "/home/yuval/Documents/Projects/RAG/examples/example_comprehensive_evaluation.py", line 189, in demonstrate_evaluation_manager
    manager = EvaluationManager(
        enable_custom=True,
        enable_deepeval=True,  # Will gracefully handle if not available
        enable_ragas=True      # Will gracefully handle if not available
    )
  File "/home/yuval/Documents/Projects/RAG/examples/../src/rag_engine/evaluation/evaluation_manager.py", line 154, in __init__
    raise RuntimeError("No evaluation frameworks could be initialized")
RuntimeError: No evaluation frameworks could be initialized

RETURN CODE: 0

COMPLETED: 2025-08-16T10:55:04.924738
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_embedding_providers.py
TIME: 2025-08-16T10:55:05.924889
================================================================================
STDOUT:
Embedding Providers Example
==================================================

=== Provider Factory Example ===
Available providers: ['openai', 'huggingface']

Testing openai provider...
Created openai provider successfully
Model info: {'provider': 'openai', 'model': 'text-embedding-ada-002', 'dimensions': 1536, 'max_input_tokens': 8191}

Testing huggingface provider...
Could not create huggingface provider: sentence-transformers package not installed. Install with: pip install sentence-transformers

=== Configuration Integration Example ===
Configuration: openai - text-embedding-ada-002
Created provider from config: {'provider': 'openai', 'model': 'text-embedding-ada-002', 'dimensions': 1536, 'max_input_tokens': 8191}

=== Custom Provider Example ===
Custom provider info: {'provider': 'mock', 'model': 'random-embeddings', 'dimensions': 256}
Generated 2 embeddings with dimension 256
Query embedding dimension: 256
=== OpenAI Embedding Provider Example ===
Provider info: {'provider': 'openai', 'model': 'text-embedding-ada-002', 'dimensions': 1536, 'max_input_tokens': 8191}
Embedding dimension: 1536

Generating document embeddings...
Error with OpenAI provider: Failed to generate document embeddings: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

Skipping HuggingFace example (sentence-transformers package not installed)

==================================================
Example completed!

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Could not determine embedding dimension, using default: 1536
Could not determine embedding dimension, using default: 1536
Could not determine embedding dimension, using default: 1536

RETURN CODE: 0

COMPLETED: 2025-08-16T10:55:08.377531
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_evaluation_demo.py
TIME: 2025-08-16T10:55:09.377696
================================================================================
STDOUT:
üîç EVALUATION FRAMEWORK DEMONSTRATION
================================================================================
This example demonstrates the evaluation framework capabilities
without requiring API keys or external dependencies.

üìã Components demonstrated:
‚Ä¢ Evaluation data models (TestCase, RAGResponse, EvaluationResult)
‚Ä¢ Multiple evaluation framework result structures
‚Ä¢ MetricsCollector for aggregating results
‚Ä¢ Comprehensive reporting and analysis
‚Ä¢ JSON export capabilities
================================================================================
EVALUATION DATA MODELS DEMONSTRATION
================================================================================
üìã Created 2 test cases:
  1. What is the capital of France?
     Expected: Paris
     Category: geography
  2. Who wrote '1984'?
     Expected: George Orwell
     Category: literature

ü§ñ Created 2 RAG responses:
  1. Paris is the capital and largest city of France.
     Confidence: 0.95
     Sources: 1
     Time: 0.120s
  2. George Orwell wrote the dystopian novel '1984'.
     Confidence: 0.92
     Sources: 1
     Time: 0.150s

================================================================================
EVALUATION RESULTS DEMONSTRATION
================================================================================
üìä EVALUATION RESULTS FROM DIFFERENT FRAMEWORKS:
------------------------------------------------------------

CUSTOM FRAMEWORK:
  Overall Score: 0.85
  Metrics: ['faithfulness', 'correctness', 'contextual_relevancy']
  Test Cases: 2
  Recommendations: 3
  Top Metrics: faithfulness=0.90, contextual_relevancy=0.83

DEEPEVAL FRAMEWORK:
  Overall Score: 0.78
  Metrics: ['correctness', 'faithfulness', 'contextual_relevancy']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.82, correctness=0.80

RAGAS FRAMEWORK:
  Overall Score: 0.82
  Metrics: ['faithfulness', 'answer_relevancy', 'context_recall', 'answer_correctness']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.88, context_recall=0.85

================================================================================
METRICS COLLECTOR DEMONSTRATION
================================================================================

================================================================================
EVALUATION RESULTS DEMONSTRATION
================================================================================
üìä EVALUATION RESULTS FROM DIFFERENT FRAMEWORKS:
------------------------------------------------------------

CUSTOM FRAMEWORK:
  Overall Score: 0.85
  Metrics: ['faithfulness', 'correctness', 'contextual_relevancy']
  Test Cases: 2
  Recommendations: 3
  Top Metrics: faithfulness=0.90, contextual_relevancy=0.83

DEEPEVAL FRAMEWORK:
  Overall Score: 0.78
  Metrics: ['correctness', 'faithfulness', 'contextual_relevancy']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.82, correctness=0.80

RAGAS FRAMEWORK:
  Overall Score: 0.82
  Metrics: ['faithfulness', 'answer_relevancy', 'context_recall', 'answer_correctness']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.88, context_recall=0.85

üìä AGGREGATED METRICS ACROSS ALL FRAMEWORKS:
--------------------------------------------------
  Faithfulness: 0.867
  Correctness: 0.810
  Contextual Relevancy: 0.775
  Answer Relevancy: 0.790
  Context Recall: 0.850
  Answer Correctness: 0.760

üîß FRAMEWORK COMPARISON:
--------------------------------------------------

  CUSTOM:
    Status: success
    Overall Score: 0.85
    Execution Time: 1.5s
    Test Cases: 2

  DEEPEVAL:
    Status: success
    Overall Score: 0.78
    Execution Time: 2.1s
    Test Cases: 1

  RAGAS:
    Status: success
    Overall Score: 0.82
    Execution Time: 3.2s
    Test Cases: 1

  FAILED_FRAMEWORK:
    Status: error
    Overall Score: 0.00
    Execution Time: 0.5s
    Test Cases: 0
    Error: Connection timeout

================================================================================
COMPREHENSIVE EVALUATION REPORTING
================================================================================

================================================================================
METRICS COLLECTOR DEMONSTRATION
================================================================================

================================================================================
EVALUATION RESULTS DEMONSTRATION
================================================================================
üìä EVALUATION RESULTS FROM DIFFERENT FRAMEWORKS:
------------------------------------------------------------

CUSTOM FRAMEWORK:
  Overall Score: 0.85
  Metrics: ['faithfulness', 'correctness', 'contextual_relevancy']
  Test Cases: 2
  Recommendations: 3
  Top Metrics: faithfulness=0.90, contextual_relevancy=0.83

DEEPEVAL FRAMEWORK:
  Overall Score: 0.78
  Metrics: ['correctness', 'faithfulness', 'contextual_relevancy']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.82, correctness=0.80

RAGAS FRAMEWORK:
  Overall Score: 0.82
  Metrics: ['faithfulness', 'answer_relevancy', 'context_recall', 'answer_correctness']
  Test Cases: 1
  Recommendations: 2
  Top Metrics: faithfulness=0.88, context_recall=0.85

üìä AGGREGATED METRICS ACROSS ALL FRAMEWORKS:
--------------------------------------------------
  Faithfulness: 0.867
  Correctness: 0.810
  Contextual Relevancy: 0.775
  Answer Relevancy: 0.790
  Context Recall: 0.850
  Answer Correctness: 0.760

üîß FRAMEWORK COMPARISON:
--------------------------------------------------

  CUSTOM:
    Status: success
    Overall Score: 0.85
    Execution Time: 1.5s
    Test Cases: 2

  DEEPEVAL:
    Status: success
    Overall Score: 0.78
    Execution Time: 2.1s
    Test Cases: 1

  RAGAS:
    Status: success
    Overall Score: 0.82
    Execution Time: 3.2s
    Test Cases: 1

  FAILED_FRAMEWORK:
    Status: error
    Overall Score: 0.00
    Execution Time: 0.5s
    Test Cases: 0
    Error: Connection timeout

üìà COMPREHENSIVE EVALUATION REPORT:
============================================================

üìã EVALUATION SUMMARY:
  Timestamp: 2024-01-15 14:30:00
  Test Cases: 2
  Frameworks: custom, deepeval, ragas
  Overall Score: 0.809
  Total Time: 7.3s

üìä AGGREGATED METRICS:
  Faithfulness: 0.867
  Correctness: 0.810
  Contextual Relevancy: 0.775
  Answer Relevancy: 0.790
  Context Recall: 0.850
  Answer Correctness: 0.760

‚ö° PERFORMANCE ANALYSIS:
  Fastest Framework: custom
  Most Accurate: custom

üí° RECOMMENDATIONS:
  1. üéØ Overall system performance is strong across multiple evaluation frameworks
  2. ‚ö° Custom framework provides fastest evaluation times
  3. üìä RAGAS offers most comprehensive RAG-specific metrics
  4. üîß Consider addressing failed framework connection issues
  5. üìà Faithfulness scores are consistently high - low hallucination risk

üíæ REPORT EXPORT (JSON):
------------------------------
{
  "summary": {
    "timestamp": "2024-01-15 14:30:00",
    "total_test_cases": 2,
    "frameworks_used": [
      "custom",
      "deepeval",
      "ragas"
    ],
    "overall_score": 0.8086111111111111,
    "total_execution_time": 7.300000000000001
  },
  "metrics": {
    "faithfulness": 0.8666666666666667,
    "correctness": 0.81,
    "contextual_relevancy": 0.7749999999999999,
    "answer_relevancy": 0.79,
    "context_recall": 0.85,
    "answer_correctness": 0.76
  }
}

================================================================================
‚úÖ DEMONSTRATION COMPLETE
================================================================================
üöÄ Key takeaways:
‚Ä¢ The evaluation framework supports multiple evaluation approaches
‚Ä¢ Results can be aggregated and compared across frameworks
‚Ä¢ Comprehensive reporting provides actionable insights
‚Ä¢ The system gracefully handles framework failures
‚Ä¢ All data structures are JSON-serializable for export

üìñ Next steps:
1. Set up API keys for actual LLM-based evaluation
2. Install optional frameworks (deepeval, ragas)
3. Run example_comprehensive_evaluation.py for full demo
4. Integrate evaluation into your RAG pipeline

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
WARNING:root:DeepEval not available. Install with: pip install deepeval
WARNING:root:RAGAS not available. Install with: pip install ragas datasets

RETURN CODE: 0

COMPLETED: 2025-08-16T10:55:10.498846
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_grounded_generation.py
TIME: 2025-08-16T10:55:11.498980
================================================================================
STDOUT:
GROUNDED GENERATION FEATURES DEMONSTRATION
This example shows the enhanced generation capabilities
implementing requirement 5.4 for grounded responses.
============================================================
BASIC vs GROUNDED GENERATION COMPARISON
============================================================
Error in basic generation demonstration: Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

============================================================
CITATION GENERATION DEMONSTRATION
============================================================
Error in citation demonstration: Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

============================================================
RESPONSE VALIDATION DEMONSTRATION
============================================================
Error in validation demonstration: Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

============================================================
FULL GROUNDING WORKFLOW DEMONSTRATION
============================================================
Error in full grounding demonstration: Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

============================================================
CITATION EXTRACTION DEMONSTRATION
============================================================
Error in citation extraction demonstration: Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

============================================================
SUMMARY
============================================================
The enhanced GenerationEngine provides:
‚Ä¢ Grounded generation with strict context adherence
‚Ä¢ Citation generation with source attribution
‚Ä¢ Response validation against retrieved documents
‚Ä¢ Comprehensive metadata and confidence scoring
‚Ä¢ Full integration with existing RAG pipeline

These features implement requirement 5.4:
'WHEN generation occurs THEN the system SHALL ensure
responses are grounded in retrieved context'

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
Failed to create google provider: Failed to initialize Google LLM provider: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

RETURN CODE: 0

COMPLETED: 2025-08-16T10:56:12.508490
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_hybrid_retrieval.py
TIME: 2025-08-16T10:56:13.508667
================================================================================
STDOUT:
HYBRID RETRIEVAL SYSTEM DEMONSTRATION
=====================================
This example demonstrates the hybrid retrieval capabilities
including keyword search, vector search combination, and
adaptive strategy selection for different query types.

============================================================
KEYWORD RETRIEVAL DEMONSTRATION
============================================================

Index Statistics:
- Total documents: 8
- Vocabulary size: 130
- Average document length: 23.1 tokens

Query: 'machine learning algorithms'

Query: 'neural networks deep learning'
  1. Score: 3.7936
     ID: dl_networks
     Content: Deep learning neural networks use multiple layers of interconnected nodes to process and learn from ...


Query: 'natural language processing'
  1. Score: 5.6266
     ID: nlp_overview
     Content: Natural language processing (NLP) is a branch of artificial intelligence that helps computers unders...


Query: 'computer vision images'
  1. Score: 4.5725
     ID: cv_algorithms
     Content: Computer vision algorithms enable machines to interpret and understand visual information from the w...


Query: 'supervised learning labeled data'
  1. Score: 0.9395
     ID: supervised_learning
     Content: Supervised learning algorithms learn from labeled training data to make predictions on new, unseen d...


============================================================
HYBRID RETRIEVAL DEMONSTRATION
============================================================

Query: 'machine learning algorithms for data analysis'
--------------------------------------------------
Hybrid Retrieval Results:
  1. ID: ml_definition
     Topic: ML
     Content: Machine learning is a subset of artificial intelligence that focuses on algorith...

  2. ID: supervised_learning
     Topic: supervised
     Content: Supervised learning algorithms learn from labeled training data to make predicti...


Query: 'deep neural networks and artificial intelligence'
--------------------------------------------------
Hybrid Retrieval Results:
  1. ID: dl_networks
     Topic: DL
     Content: Deep learning neural networks use multiple layers of interconnected nodes to pro...

  2. ID: ml_definition
     Topic: ML
     Content: Machine learning is a subset of artificial intelligence that focuses on algorith...

  3. ID: nlp_overview
     Topic: NLP
     Content: Natural language processing (NLP) is a branch of artificial intelligence that he...


Query: 'natural language processing techniques'
--------------------------------------------------
Hybrid Retrieval Results:
  1. ID: nlp_overview
     Topic: NLP
     Content: Natural language processing (NLP) is a branch of artificial intelligence that he...

  2. ID: unsupervised_learning
     Topic: unsupervised
     Content: Unsupervised learning finds hidden patterns in data without labeled examples. Cl...

  3. ID: ml_definition
     Topic: ML
     Content: Machine learning is a subset of artificial intelligence that focuses on algorith...


Query: 'computer vision and image recognition'
--------------------------------------------------
Hybrid Retrieval Results:
  1. ID: ml_definition
     Topic: ML
     Content: Machine learning is a subset of artificial intelligence that focuses on algorith...

  2. ID: dl_networks
     Topic: DL
     Content: Deep learning neural networks use multiple layers of interconnected nodes to pro...

  3. ID: cv_algorithms
     Topic: CV
     Content: Computer vision algorithms enable machines to interpret and understand visual in...


============================================================
LONG-CONTEXT RETRIEVAL DEMONSTRATION
============================================================

Context Window Size: 500 tokens
----------------------------------------
Documents processed: 8
Documents returned: 8
Estimated tokens: 394
Retrieval time: 0.000s
Included documents:
  - ml_definition: ML
  - dl_networks: DL
  - nlp_overview: NLP
  - cv_algorithms: CV
  - rl_methodology: RL
  - data_preprocessing: preprocessing
  - supervised_learning: supervised
  - unsupervised_learning: unsupervised

Context Window Size: 1000 tokens
----------------------------------------
Documents processed: 8
Documents returned: 8
Estimated tokens: 394
Retrieval time: 0.000s
Included documents:
  - ml_definition: ML
  - dl_networks: DL
  - nlp_overview: NLP
  - cv_algorithms: CV
  - rl_methodology: RL
  - data_preprocessing: preprocessing
  - supervised_learning: supervised
  - unsupervised_learning: unsupervised

Context Window Size: 2000 tokens
----------------------------------------
Documents processed: 8
Documents returned: 8
Estimated tokens: 394
Retrieval time: 0.000s
Included documents:
  - ml_definition: ML
  - dl_networks: DL
  - nlp_overview: NLP
  - cv_algorithms: CV
  - rl_methodology: RL
  - data_preprocessing: preprocessing
  - supervised_learning: supervised
  - unsupervised_learning: unsupervised

Context Window Size: 5000 tokens
----------------------------------------
Documents processed: 8
Documents returned: 8
Estimated tokens: 394
Retrieval time: 0.000s
Included documents:
  - ml_definition: ML
  - dl_networks: DL
  - nlp_overview: NLP
  - cv_algorithms: CV
  - rl_methodology: RL
  - data_preprocessing: preprocessing
  - supervised_learning: supervised
  - unsupervised_learning: unsupervised

============================================================
ADAPTIVE RETRIEVAL DEMONSTRATION
============================================================

Query Type: Simple semantic query
Query: 'What is AI?'
--------------------------------------------------
Selected Strategy: vector
Documents Returned: 4
Query Length: 11 characters
‚Üí Used standard vector retrieval

Query Type: Keyword-focused query
Query: 'Find specific machine learning algorithms for classification'
--------------------------------------------------
Selected Strategy: hybrid
Documents Returned: 3
Query Length: 60 characters
‚Üí Used hybrid retrieval (vector + keyword)

Query Type: Complex analytical query
Query: 'Compare and analyze different approaches to deep learning'
--------------------------------------------------
Selected Strategy: rerank
Documents Returned: 2
Query Length: 57 characters
‚Üí Used retrieval with re-ranking

Query Type: Very long query
Query: 'Provide a comprehensive detailed explanation of the entire machine learning ecosystem including supervised unsupervised reinforcement learning methodologies'
--------------------------------------------------
Selected Strategy: vector
Documents Returned: 4
Query Length: 156 characters
‚Üí Used standard vector retrieval

============================================================
DEMONSTRATION COMPLETE
============================================================
The hybrid retrieval system successfully demonstrated:
‚úì BM25-based keyword retrieval
‚úì Hybrid vector + keyword search
‚úì Long-context document filtering
‚úì Adaptive strategy selection
‚úì Comprehensive error handling

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
INFO:rag_engine.retrieval.keyword_retriever:KeywordRetriever initialized
INFO:rag_engine.retrieval.keyword_retriever:Added 8 documents to keyword index. Total documents: 8, Vocabulary size: 130
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 0 documents with scores using keyword search
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 1 documents with scores using keyword search
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 1 documents with scores using keyword search
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 1 documents with scores using keyword search
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 1 documents with scores using keyword search
INFO:rag_engine.retrieval.vector_retriever:VectorRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: vector
INFO:rag_engine.retrieval.keyword_retriever:KeywordRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: keyword
INFO:rag_engine.retrieval.retrieval_engine:RetrievalEngine initialized
INFO:rag_engine.retrieval.keyword_retriever:Added 8 documents to keyword index. Total documents: 8, Vocabulary size: 130
INFO:rag_engine.retrieval.retrieval_engine:Added 8 documents to 2 retrievers
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 0 documents with scores using keyword search
INFO:rag_engine.retrieval.retrieval_engine:Hybrid retrieved 2 documents in 0.000s
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 3 documents with scores using keyword search
INFO:rag_engine.retrieval.retrieval_engine:Hybrid retrieved 3 documents in 0.000s
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 2 documents with scores using keyword search
INFO:rag_engine.retrieval.retrieval_engine:Hybrid retrieved 3 documents in 0.000s
INFO:rag_engine.retrieval.keyword_retriever:Retrieved 1 documents with scores using keyword search
INFO:rag_engine.retrieval.retrieval_engine:Hybrid retrieved 3 documents in 0.000s
INFO:rag_engine.retrieval.vector_retriever:VectorRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: vector
INFO:rag_engine.retrieval.keyword_retriever:KeywordRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: keyword
INFO:rag_engine.retrieval.retrieval_engine:RetrievalEngine initialized
INFO:rag_engine.retrieval.keyword_retriever:Added 8 documents to keyword index. Total documents: 8, Vocabulary size: 130
INFO:rag_engine.retrieval.retrieval_engine:Added 8 documents to 2 retrievers
INFO:rag_engine.retrieval.retrieval_engine:Long-context retrieved 8 documents (394 tokens) in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Long-context retrieved 8 documents (394 tokens) in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Long-context retrieved 8 documents (394 tokens) in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Long-context retrieved 8 documents (394 tokens) in 0.000s
INFO:rag_engine.retrieval.vector_retriever:VectorRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: vector
INFO:rag_engine.retrieval.keyword_retriever:KeywordRetriever initialized
INFO:rag_engine.retrieval.retrieval_engine:Added retriever: keyword
INFO:rag_engine.retrieval.retrieval_engine:RetrievalEngine initialized
INFO:rag_engine.retrieval.keyword_retriever:Added 8 documents to keyword index. Total documents: 8, Vocabulary size: 130
INFO:rag_engine.retrieval.retrieval_engine:Added 8 documents to 2 retrievers
INFO:rag_engine.retrieval.retrieval_engine:Adaptive retrieval using vector returned 4 documents in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Adaptive retrieval using hybrid returned 3 documents in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Adaptive retrieval using rerank returned 2 documents in 0.000s
INFO:rag_engine.retrieval.retrieval_engine:Adaptive retrieval using vector returned 4 documents in 0.000s

RETURN CODE: 0

COMPLETED: 2025-08-16T10:56:14.655637
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_integration_test.py
TIME: 2025-08-16T10:56:15.655775
================================================================================
STDOUT:
GROUNDED GENERATION INTEGRATION TESTS
Testing all components working together
============================================================
============================================================
BASIC GROUNDED GENERATION TEST
============================================================
Query: What is Python?
Documents: 3
Grounded Response: Python is a high-level, interpreted programming language created by Guido van Rossum in 1991 (as stated in the context). It emphasizes code readability and uses significant whitespace.
‚úÖ Basic grounded generation successful
‚úÖ Grounding instructions applied successfully

============================================================
CITATION GENERATION TEST
============================================================
Query: What is Python and machine learning?
Response with Citations: Python is a high-level, interpreted programming language that was created by Guido van Rossum in 1991 [1]. It is designed to emphasize code readability and uses significant whitespace [1]. Machine learning is a subset of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed [2].
Source Metadata Count: 3
Citations Found: [1, 1, 2]
Citation [1]: Python Language Overview
Citation [2]: Introduction to Machine Learning
‚úÖ Citation generation successful

============================================================
RESPONSE VALIDATION TEST
============================================================
Query: What is Python?
Response: Python is a programming language created by Guido van Rossum in 1991.
Validation Results:
  Grounded: YES
  Confidence: 1.0
  Issues: []
  Explanation: The answer accurately reflects information present in the context.
‚ùå Response validation failed: 

============================================================
FULL GROUNDING WORKFLOW TEST
============================================================
Query: What is Python?
Response Type: RAGResponse
Answer: Python is a high-level, interpreted programming language [1]. It was created by Guido van Rossum in 1991 [1]. Python emphasizes code readability and uses significant whitespace [1].
Confidence Score: 1.0
Source Documents: 3
Processing Time: 5.2465s
Metadata Keys: ['grounding_applied', 'citations_included', 'validation_applied', 'source_count', 'processing_time', 'source_metadata', 'validation_results']
Grounding Applied: True
Citations Included: True
Validation Applied: True
‚úÖ Full grounding workflow successful

============================================================
SELF-CORRECTION INTEGRATION TEST
============================================================
Query: What is Python?
Response: Python is a high-level, interpreted programming language. It was created by Guido van Rossum in 1991. Python emphasizes code readability and uses significant whitespace.

Note: This response is based on the provided context. Some details may require additional verification.
Validation Applied: True
Validation Grade: partially_grounded
‚úÖ Self-correction integration successful

============================================================
EVALUATION FRAMEWORK INTEGRATION TEST
============================================================
Test Cases: 1
Responses: 1
‚úÖ Custom evaluator initialized
üìä Supported metrics: ['correctness', 'faithfulness', 'contextual_relevancy']
‚úÖ Evaluation manager initialized
üîß Available frameworks: ['custom']
üìã Framework capabilities:
   ‚Ä¢ custom: 3 metrics
‚úÖ Evaluation framework integration successful
üìù Note: Actual evaluation requires LLM API keys and would make real API calls

============================================================
INTEGRATION TEST RESULTS
============================================================
Tests Passed: 5/6
‚ùå 1 tests failed

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Could not load prompt from hub: Failed to GET /commits/rlm/rag-prompt/latest in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/commits/rlm/rag-prompt/latest', '{"error":"Forbidden"}\n'), using default
Could not load prompt from hub: Failed to GET /commits/rlm/rag-prompt/latest in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/commits/rlm/rag-prompt/latest', '{"error":"Forbidden"}\n'), using default
Could not load prompt from hub: Failed to GET /commits/rlm/rag-prompt/latest in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/commits/rlm/rag-prompt/latest', '{"error":"Forbidden"}\n'), using default
Could not load prompt from hub: Failed to GET /commits/rlm/rag-prompt/latest in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/commits/rlm/rag-prompt/latest', '{"error":"Forbidden"}\n'), using default
Could not load prompt from hub: Failed to GET /commits/rlm/rag-prompt/latest in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/commits/rlm/rag-prompt/latest', '{"error":"Forbidden"}\n'), using default
Error validating response: 1 validation error for Generation
text
  Input should be a valid string [type=string_type, input_value=<MagicMock name='ChatGoog...)' id='140025109019152'>, input_type=MagicMock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
WARNING:root:DeepEval not available. Install with: pip install deepeval
WARNING:root:RAGAS not available. Install with: pip install ragas datasets
WARNING:root:DeepEval requested but not available
WARNING:root:RAGAS requested but not available

RETURN CODE: 1

COMPLETED: 2025-08-16T10:56:27.571738
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_multi_provider_llm.py
TIME: 2025-08-16T10:56:28.571919
================================================================================
STDOUT:
üöÄ MULTI-PROVIDER LLM SUPPORT DEMONSTRATION
================================================================================
This example demonstrates the multi-provider LLM support system
that allows switching between Google Gemini, OpenAI, Anthropic, and local models.
================================================================================

üè≠ LLM PROVIDER FACTORY DEMONSTRATION
================================================================================
üìã Available LLM providers: google, openai, anthropic, local, ollama

üîß GOOGLE Provider Configuration:
   ‚Ä¢ Models: gemini-2.0-flash-lite, gemini-1.5-pro, gemini-1.5-flash, gemini-pro
   ‚Ä¢ Default Model: gemini-2.0-flash-lite
   ‚Ä¢ Structured Output: True
   ‚Ä¢ API Key Environment: GOOGLE_API_KEY

üîß OPENAI Provider Configuration:
   ‚Ä¢ Models: gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o
   ‚Ä¢ Default Model: gpt-3.5-turbo
   ‚Ä¢ Structured Output: True
   ‚Ä¢ API Key Environment: OPENAI_API_KEY

üîß ANTHROPIC Provider Configuration:
   ‚Ä¢ Models: claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229
   ‚Ä¢ Default Model: claude-3-sonnet-20240229
   ‚Ä¢ Structured Output: True
   ‚Ä¢ API Key Environment: ANTHROPIC_API_KEY

üîß LOCAL Provider Configuration:
   ‚Ä¢ Models: llama2, mistral, codellama, custom
   ‚Ä¢ Default Model: llama2
   ‚Ä¢ Structured Output: False
   ‚Ä¢ API Key Environment: None

ü§ñ GOOGLE GEMINI PROVIDER DEMONSTRATION
================================================================================
‚úÖ Configuration is valid
üìã Configuration:
   ‚Ä¢ Provider: google
   ‚Ä¢ Model: gemini-2.0-flash-lite
   ‚Ä¢ Temperature: 0.7
   ‚Ä¢ Max Tokens: 1000

‚úÖ Google provider created successfully
üìä Model Info: {'provider': 'google', 'model': 'gemini-2.0-flash-lite', 'temperature': 0.7, 'max_tokens': 1000, 'supports_structured_output': True}

ü§ñ OPENAI PROVIDER DEMONSTRATION
================================================================================
‚ö†Ô∏è  Configuration issues found:
   ‚Ä¢ API key required for openai provider. Set OPENAI_API_KEY environment variable or config.openai_api_key
   Note: This is expected if OPENAI_API_KEY is not set
üìã Configuration:
   ‚Ä¢ Provider: openai
   ‚Ä¢ Model: gpt-3.5-turbo
   ‚Ä¢ Temperature: 0.5
   ‚Ä¢ Max Tokens: 500

‚ö†Ô∏è  Provider creation failed (expected without API key): Failed to create openai provider: Failed to initialize OpenAI LLM provider: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

ü§ñ LOCAL/OLLAMA PROVIDER DEMONSTRATION
================================================================================
‚úÖ Configuration is valid
üìã Configuration:
   ‚Ä¢ Provider: local
   ‚Ä¢ Model: llama2
   ‚Ä¢ Temperature: 0.3
   ‚Ä¢ Base URL: http://localhost:11434

‚úÖ Local provider created successfully
üìä Model Info: {'provider': 'local', 'model': 'llama2', 'temperature': 0.3, 'max_tokens': None, 'supports_structured_output': False, 'base_url': 'http://localhost:11434'}

üîÑ PROVIDER SWITCHING DEMONSTRATION
================================================================================
üîß Testing google provider with gemini-2.0-flash-lite...
   ‚úÖ Success: google - gemini-2.0-flash-lite

üîß Testing openai provider with gpt-3.5-turbo...
   ‚ö†Ô∏è  Failed: Failed to create openai provider: Failed to initialize OpenAI LLM provider: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

üîß Testing local provider with llama2...
   ‚úÖ Success: local - llama2

üîß CUSTOM PROVIDER DEMONSTRATION
================================================================================
‚úÖ Registered custom 'mock' provider
   MockLLMProvider initialized with model: mock-model-v1
‚úÖ Created mock provider instance
üìù Generated response: Mock response to: What is the meaning of life?...
üìä Structured output: {'mock': 'structured response', 'prompt': 'Generate a summary'}
üìã Model info: {'provider': 'mock', 'model': 'mock-model-v1', 'temperature': 0.8, 'supports_structured_output': True}

üéâ MULTI-PROVIDER LLM DEMONSTRATION COMPLETE!
================================================================================
Key Features Demonstrated:
‚Ä¢ ‚úÖ Provider factory with multiple LLM backends
‚Ä¢ ‚úÖ Configuration validation and environment variable support
‚Ä¢ ‚úÖ Google Gemini, OpenAI, Anthropic, and Local provider support
‚Ä¢ ‚úÖ Dynamic provider switching
‚Ä¢ ‚úÖ Custom provider registration
‚Ä¢ ‚úÖ Structured output support
‚Ä¢ ‚úÖ Model information and capabilities

üí° To use with real API keys, set the following environment variables:
   ‚Ä¢ GOOGLE_API_KEY for Google Gemini
   ‚Ä¢ OPENAI_API_KEY for OpenAI
   ‚Ä¢ ANTHROPIC_API_KEY for Anthropic Claude
   ‚Ä¢ OLLAMA_BASE_URL for local Ollama server

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
Failed to create openai provider: Failed to initialize OpenAI LLM provider: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
/home/yuval/Documents/Projects/RAG/src/rag_engine/generation/llm_providers.py:371: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.
  self.llm = Ollama(**init_params)
Failed to create openai provider: Failed to initialize OpenAI LLM provider: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

RETURN CODE: 0

COMPLETED: 2025-08-16T10:56:29.900627
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_self_correction.py
TIME: 2025-08-16T10:56:30.900818
================================================================================
STDOUT:
SELF-CORRECTION MECHANISMS DEMONSTRATION
Based on CRAG and Self-RAG approaches from workplan/04AdvancedRetrieval-Generation.md

============================================================
CRAG RELEVANCE CHECKING DEMONSTRATION
============================================================
Initializing CRAG relevance checker...
‚ö† CRAG relevance checker initialization failed (expected without credentials): DefaultCredentialsError
  Proceeding with simulated functionality for demonstration...

Query: What is machine learning and how does it work?

Assessing document relevance...

Document 1 (machine_learning):
Content preview: Machine learning is a subset of artificial intelligence (AI) that focuses on the development 
      ...
  Grade: relevant
  Confidence: 0.90
  Reasoning: Document is about machine_learning which is directly relevant to the query

Document 2 (weather):
Content preview: The weather forecast for tomorrow shows sunny skies with a high temperature of 75¬∞F and 
           ...
  Grade: irrelevant
  Confidence: 0.80
  Reasoning: Document is about weather which is not relevant to machine learning

Document 3 (deep_learning):
Content preview: Deep learning is a subset of machine learning that uses artificial neural networks with 
           ...
  Grade: relevant
  Confidence: 0.85
  Reasoning: Document is about deep_learning which is directly relevant to the query

Document 4 (programming):
Content preview: Python is a high-level, interpreted programming language known for its simplicity and 
            r...
  Grade: ambiguous
  Confidence: 0.60
  Reasoning: Document is about programming which is somewhat related but not directly answering the question

Document 5 (finance):
Content preview: The stock market experienced significant volatility today with the Dow Jones dropping 
            2...
  Grade: irrelevant
  Confidence: 0.80
  Reasoning: Document is about finance which is not relevant to machine learning

Filtering documents with relevance threshold: 0.7
‚úì Filtered 2 relevant documents from 5 total
  Relevant Doc 1: machine_learning
  Relevant Doc 2: deep_learning
‚úì Sufficient relevant documents found (>= 2)

============================================================
SELF-RAG RESPONSE VALIDATION DEMONSTRATION
============================================================
Initializing Self-RAG validator...
‚ö† Self-RAG validator initialization failed (expected without credentials): DefaultCredentialsError
  Proceeding with simulated functionality for demonstration...

Test Case 1: Well-grounded response
Query: What is machine learning?
Response: Machine learning is a subset of AI that uses algorithms to learn from data and make predictions.
  Validation Result:
    Grade: grounded
    Confidence: 0.90
    Citations Found: True
    Reasoning: Response is fully supported by the provided context
  ‚úì No correction needed

Test Case 2: Not grounded response with false claims
Query: What is machine learning?
Response: Machine learning was invented by aliens in 1950 and uses quantum computers to predict the future.
  Validation Result:
    Grade: not_grounded
    Confidence: 0.85
    Citations Found: False
    Reasoning: Response contains claims not supported by the context
  ‚ö† Correction needed (confidence 0.85 < threshold 0.7)
  Corrected Response: Based on the provided context, I cannot fully verify all claims in the original response. Here's wha...

Test Case 3: Partially grounded response
Query: What is machine learning?
Response: Machine learning is related to AI and involves some form of data processing, though the exact mechanisms are complex.
  Validation Result:
    Grade: partially_grounded
    Confidence: 0.60
    Citations Found: False
    Reasoning: Response is somewhat supported but lacks specific details from context
  ‚ö† Correction needed (confidence 0.60 < threshold 0.7)
  Corrected Response: Machine learning is related to AI and involves some form of data processing, though the exact mechan...

============================================================
FULL SELF-CORRECTION PIPELINE DEMONSTRATION
============================================================
Initializing self-correction engine...
‚ö† Self-correction engine initialization failed (expected without credentials): DefaultCredentialsError
  Proceeding with simulated functionality for demonstration...

Original Query: What is machine learning and how does it relate to AI?
Original Documents: 5 documents
Original Response: Machine learning is a subset of artificial intelligence that uses algorithms to learn from data.

Applying self-correction pipeline...
  Retrieval Correction:
    Original documents: 5
    Filtered documents: 2
    Correction applied: True
    Fallback triggered: False
  Response Validation:
    Grade: grounded
    Confidence: 0.9
    Citations found: True
    Correction needed: False

Final Results:
  Final documents: 2
  Final response: Machine learning is a subset of artificial intelligence that uses algorithms to learn from data.
  Total corrections applied: 1

Correction Statistics:
  relevance_threshold: 0.7
  factuality_threshold: 0.7
  min_relevant_docs: 2
‚úì Self-correction pipeline completed successfully

============================================================
DEMONSTRATION COMPLETED SUCCESSFULLY
============================================================

Key Features Demonstrated:
‚Ä¢ CRAG-style relevance checking for retrieved documents
‚Ä¢ Self-RAG validation for generated responses
‚Ä¢ Fallback strategies for low-confidence results
‚Ä¢ Configurable thresholds for correction decisions
‚Ä¢ Integration with existing RAG pipeline components

Note: This demonstration uses simulated LLM responses.
In a real implementation, actual LLM calls would be made.

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.

RETURN CODE: 0

COMPLETED: 2025-08-16T10:57:07.961533
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_simple.py
TIME: 2025-08-16T10:57:08.961688
================================================================================
STDOUT:
üéØ Simple Query Processing Demo
==================================================
‚úÖ GOOGLE_API_KEY found - full functionality available!

1. üöÄ Initializing Query Processor...
‚úÖ Query Processor initialized successfully!

2. üìã Available Query Processing Strategies:
   1. MULTI_QUERY
      Description: Generates multiple alternative versions of the query for broader retrieval
      Supports Retriever: True

   2. RAG_FUSION
      Description: Combines multiple queries with Reciprocal Rank Fusion for intelligent re-ranking
      Supports Retriever: True

   3. DECOMPOSITION
      Description: Breaks complex queries into simpler sub-questions for systematic answering
      Supports Retriever: True

   4. STEP_BACK
      Description: Generates broader, more general questions to provide richer context
      Supports Retriever: True

   5. HYDE
      Description: Creates hypothetical documents to improve semantic matching during retrieval
      Supports Retriever: True

   6. BASIC
      Description: Basic query processing without transformation
      Supports Retriever: False

3. üîç Basic Query Processing:
   Query: What is task decomposition for LLM agents?
   ‚úÖ Basic processing successful!
   Original: What is task decomposition for LLM agents?
   Strategy: basic
   Transformed: ['What is task decomposition for LLM agents?']

4. üîß Configuration Management:
   Current configurations:
     multi_query: multi_query
     rag_fusion: rag_fusion
     decomposition: decomposition
     step_back: step_back
     hyde: hyde
     basic: basic
   ‚úÖ Successfully configured multi_query strategy

5. üìö Sample Documents and Retrieval:
   Created 3 sample documents
   Retrieved 3 documents for query
     1. Task decomposition breaks complex problems into smaller sub-...
     2. LLM agents use planning to organize problem-solving approach...
     3. Memory systems help agents retain information across interac...

6. ‚ö†Ô∏è  Error Handling:
   ‚úÖ Properly caught invalid strategy error: QueryProcessingError
   ‚úÖ Properly caught invalid config error: QueryProcessingError

7. üìä Summary:
   ‚úÖ Query Processor initialized with 6 strategies
   ‚úÖ Basic query processing works without API keys
   ‚úÖ Configuration management functional
   ‚úÖ Error handling working properly
   ‚úÖ Ready for integration with real retrievers

üí° Next Steps:
   1. Set GOOGLE_API_KEY to use advanced strategies
   2. Integrate with your document store and retriever
   3. Choose appropriate strategies for your use case
   4. Configure strategies for optimal performance

üéâ Demo completed successfully!

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.

RETURN CODE: 0

COMPLETED: 2025-08-16T10:57:10.175110
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_usage.py
TIME: 2025-08-16T10:57:11.175264
================================================================================
STDOUT:
üöÄ Advanced Query Processing - Usage Examples
This demo shows practical usage patterns
‚úÖ GOOGLE_API_KEY found - full functionality available!
üéØ Strategy Comparison Demo
==================================================
‚úÖ RAG System initialized successfully!

============================================================
Testing BASIC Strategy
============================================================
ü§î Question: How do LLM agents use task decomposition and planning for complex problem solving?
üîÑ Using strategy: BASIC
üìù Result:
   ‚úÖ Strategy: basic
   üìö Documents used: 3
      1. task_decomposition - Task decomposition is a fundamental approach in AI where complex problems are sy...
      2. planning - Large Language Model (LLM) agents utilize advanced planning mechanisms to organi...
   üí¨ Answer: Basic answer using 3 documents...

============================================================
Testing MULTI_QUERY Strategy
============================================================
ü§î Question: How do LLM agents use task decomposition and planning for complex problem solving?
üîÑ Using strategy: MULTI_QUERY
üìö Retrieved 3 documents
üìù Result:
   ‚úÖ Strategy: multi_query
   üìö Documents used: 3
      1. task_decomposition - Task decomposition is a fundamental approach in AI where complex problems are sy...
      2. planning - Large Language Model (LLM) agents utilize advanced planning mechanisms to organi...
   üí¨ Answer: Based on the retrieved documents: Source: ai_fundamentals.pdf
Task decomposition is a fundamental approach in AI where complex problems are systematic...

============================================================
Testing RAG_FUSION Strategy
============================================================
ü§î Question: How do LLM agents use task decomposition and planning for complex problem solving?
üîÑ Using strategy: RAG_FUSION
üìä Retrieved 3 documents with RRF scores: ['0.050', '0.033', '0.033']
üìù Result:
   ‚úÖ Strategy: rag_fusion
   üìö Documents used: 3
      1. task_decomposition - Task decomposition is a fundamental approach in AI where complex problems are sy...
      2. memory - Memory systems in AI agents serve as crucial components for maintaining context ...
   üí¨ Answer: Based on the retrieved documents: Source: ai_fundamentals.pdf
Task decomposition is a fundamental approach in AI where complex problems are systematic...

============================================================
Testing STEP_BACK Strategy
============================================================
ü§î Question: How do LLM agents use task decomposition and planning for complex problem solving?
üîÑ Using strategy: STEP_BACK
üîô Step-back question: what are the general strategies for solving complex problems using AI?
üìö Normal context: 3 docs, Step-back context: 3 docs
üìù Result:
   ‚úÖ Strategy: step_back
   üîô Step-back question: what are the general strategies for solving complex problems using AI?
   üí¨ Answer: Combined answer using both normal and step-back context...

============================================================
Testing HYDE Strategy
============================================================
ü§î Question: How do LLM agents use task decomposition and planning for complex problem solving?
üîÑ Using strategy: HYDE
üìö Retrieved 3 documents
üìù Result:
   ‚úÖ Strategy: hyde
   üìö Documents used: 3
      1. task_decomposition - Task decomposition is a fundamental approach in AI where complex problems are sy...
      2. planning - Large Language Model (LLM) agents utilize advanced planning mechanisms to organi...
   üí¨ Answer: Based on the retrieved documents: Source: ai_fundamentals.pdf
Task decomposition is a fundamental approach in AI where complex problems are systematic...

üîß Configuration Demo
==============================
üìã Available strategies:
   ‚Ä¢ multi_query: Generates multiple alternative versions of the query for bro...
   ‚Ä¢ rag_fusion: Combines multiple queries with Reciprocal Rank Fusion for in...
   ‚Ä¢ decomposition: Breaks complex queries into simpler sub-questions for system...
   ‚Ä¢ step_back: Generates broader, more general questions to provide richer ...
   ‚Ä¢ hyde: Creates hypothetical documents to improve semantic matching ...
   ‚Ä¢ basic: Basic query processing without transformation...

‚öôÔ∏è  Configuring strategies:
   ‚úÖ Configured multi_query and hyde strategies

üìä Current configurations:
   multi_query: multi_query
   rag_fusion: rag_fusion
   decomposition: decomposition
   step_back: step_back
   hyde: hyde
   basic: basic

============================================================
‚úÖ Usage demonstration completed!

üí° Key Usage Patterns:
  1. Initialize QueryProcessor with your preferred settings
  2. Configure strategies for your specific use case
  3. Choose strategy based on query complexity and requirements
  4. Integrate with your existing retriever and knowledge base
  5. Handle errors gracefully in production

üéØ Strategy Selection Guide:
  ‚Ä¢ multi_query: Good for broad topic coverage
  ‚Ä¢ rag_fusion: Best for precise ranking and relevance
  ‚Ä¢ decomposition: Ideal for complex, multi-part questions
  ‚Ä¢ step_back: Useful when queries are too specific
  ‚Ä¢ hyde: Excellent for semantic similarity matching
  ‚Ä¢ basic: Simple pass-through for basic use cases

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.

RETURN CODE: 0

COMPLETED: 2025-08-16T10:57:25.276827
--------------------------------------------------------------------------------

================================================================================
RUNNING: example_vector_store_providers.py
TIME: 2025-08-16T10:57:26.276983
================================================================================
STDOUT:
Vector Store Providers Demo
==================================================

=== Chroma Provider Demo ===
‚úì Initialized Chroma provider
‚úì Store info: chroma with 0 documents
‚úì Chroma provider demo completed (mocked operations)

=== Pinecone Provider Demo ===
‚úó Pinecone demo failed: Pinecone initialization failed: Index creation failed: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'c1a9d863ce6cb8e2ce2db213f6575dca', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key


=== Weaviate Provider Demo ===
‚úó Weaviate demo failed: Weaviate initialization failed: Connection to Weaviate failed. Details: Error: [Errno 111] Connection refused. 
Is Weaviate running and reachable at http://localhost:8080?

=== Provider Switching Demo ===
‚úì Available providers: ['chroma', 'pinecone', 'weaviate']
‚úì Switched to chroma: chroma
‚úó Failed to switch to pinecone: Pinecone initialization failed: Index creation failed: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '9ca0697dfdd44c4be86cee4dfcd5de83', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key

‚úó Failed to switch to weaviate: Weaviate initialization failed: Connection to Weaviate failed. Details: Error: [Errno 111] Connection refused. 
Is Weaviate running and reachable at http://localhost:8080?

=== Configuration Management Demo ===
‚úì Created direct configuration
‚úì Loaded environment configuration: vector_store=pinecone
‚úì Created chroma configuration with 2 settings
‚úì Created pinecone configuration with 3 settings
‚úì Created weaviate configuration with 3 settings

==================================================
Demo completed! Check the implementations for full functionality.

To use in production:
1. Set GOOGLE_API_KEY environment variable
2. Set PINECONE_API_KEY for Pinecone usage
3. Start Weaviate service for Weaviate usage
4. Configure vector_store_config in PipelineConfig

STDERR:
USER_AGENT environment variable not set, consider setting it to identify your requests.
/home/yuval/Documents/Projects/RAG/src/rag_engine/core/providers/chroma_provider.py:39: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.
  self._store = Chroma(
Failed to ensure index exists: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'c1a9d863ce6cb8e2ce2db213f6575dca', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key

Failed to initialize Pinecone vector store: Index creation failed: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'c1a9d863ce6cb8e2ce2db213f6575dca', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key

Failed to initialize Weaviate vector store: Connection to Weaviate failed. Details: Error: [Errno 111] Connection refused. 
Is Weaviate running and reachable at http://localhost:8080?
Failed to ensure index exists: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '9ca0697dfdd44c4be86cee4dfcd5de83', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key

Failed to initialize Pinecone vector store: Index creation failed: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '9ca0697dfdd44c4be86cee4dfcd5de83', 'date': 'Sat, 16 Aug 2025 07:57:27 GMT', 'content-type': 'text/html', 'server': 'Google Frontend', 'Content-Length': '15', 'Via': '1.1 google', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'})
HTTP response body: Invalid API Key

Failed to initialize Weaviate vector store: Connection to Weaviate failed. Details: Error: [Errno 111] Connection refused. 
Is Weaviate running and reachable at http://localhost:8080?
/usr/lib64/python3.13/asyncio/base_events.py:764: ResourceWarning: unclosed event loop <_UnixSelectorEventLoop running=False closed=False debug=False>
  _warn(f"unclosed event loop {self!r}", ResourceWarning, source=self)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
<sys>:0: ResourceWarning: unclosed <ssl.SSLSocket fd=50, family=10, type=1, proto=6, laddr=('2a06:c701:73f5:e700:b4a8:ca2d:7eee:e6cc', 52066, 0, 0), raddr=('2600:1f18:4c12:9a02:478d:eb7d:40d0:7d22', 443, 0, 0)>

RETURN CODE: 0

COMPLETED: 2025-08-16T10:57:29.077991
--------------------------------------------------------------------------------

================================================================================
SUMMARY
================================================================================
Total examples: 13
Successful: 12
Failed: 1
Success rate: 92.3%
Completed: 2025-08-16T10:57:30.078127
