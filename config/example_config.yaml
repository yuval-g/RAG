# Example configuration for the RAG system
# This file demonstrates how to configure the RAG engine

# Environment Configuration
environment: "development"

# LLM Configuration
llm_provider: "google"  # google, openai, local, ollama
llm_model: "gemini-2.0-flash-lite"
temperature: 0.0
max_tokens: null

# Embedding Configuration
embedding_provider: "openai"  # openai, huggingface
embedding_model: "text-embedding-ada-002"
embedding_dimensions: null
embedding_device: null  # For HuggingFace models (cpu, cuda, mps)
normalize_embeddings: true

# Vector Store Configuration
vector_store: "chroma"  # chroma, pinecone, weaviate
vector_store_config: 
  persist_directory: "./data/chroma"
  collection_name: "rag_collection"

# Indexing Configuration
indexing_strategy: "basic"  # basic, multi_representation, colbert, raptor
chunk_size: 1000
chunk_overlap: 200

# Query Processing Configuration
query_strategies: ["basic"]
enable_multi_query: false
enable_rag_fusion: false
enable_decomposition: false
enable_step_back: false
enable_hyde: false

# Routing Configuration
routing_enabled: false
routing_strategy: "logical"  # logical, semantic, hybrid

# Retrieval Configuration
retrieval_k: 5
use_reranking: false
reranker_top_k: 10

# Hybrid Retrieval Configuration
enable_hybrid_retrieval: false
vector_weight: 0.7
keyword_weight: 0.3
enable_long_context: false
context_window_size: 100000
adaptive_retrieval: false

# Self-Correction Configuration
enable_self_correction: false
relevance_threshold: 0.7
factuality_threshold: 0.7
min_relevant_docs: 2

# Generation Configuration
prompt_template: null
include_sources: true

# Evaluation Configuration
evaluation_frameworks: ["custom"]

# Production Configuration
enable_logging: true
log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
enable_metrics: false
enable_caching: false

# API Keys (can also be set via environment variables)
google_api_key: null
openai_api_key: null
